import pandas as pd
import numpy as np
import os
from sklearn.decomposition import PCA
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler
from scipy.stats import entropy
import networkx as nx
from statsmodels.multivariate.factor import Factor

# ==========================================
# [ì„¤ì •] ì…ë ¥ ë° ì¶œë ¥ ê²½ë¡œ
# ==========================================
BASE_DIR = "Project_HighSchool_apply_Analytics"
INPUT_EXCEL = os.path.join(BASE_DIR, "data", "processed", "Step2_ì§€ë§ì„ í˜¸ë„_ë°_ì§€ì—­íë¦„.xlsx")
OUTPUT_EXCEL = os.path.join(BASE_DIR, "data", "processed", "Step4_ëŒ€í•™ì›ìˆ˜ì¤€_ì‹¬ì¸µë¶„ì„.xlsx")

def load_data():
    """Step 2ì—ì„œ ìƒì„±ëœ ì—°êµ¬ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    if not os.path.exists(INPUT_EXCEL):
        raise FileNotFoundError(f"ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {INPUT_EXCEL}")
    
    with pd.ExcelFile(INPUT_EXCEL) as xls:
        df_school = pd.read_excel(xls, 'ì—°êµ¬1_í•™êµë³„_ì¸ê¸°ë„')
        df_matrix = pd.read_excel(xls, 'ë¶€ë¡_ë™ë„¤_í•™êµ_ì „ì²´ë§¤íŠ¸ë¦­ìŠ¤', index_col=0)
    return df_school, df_matrix

def analysis_pca_factor(df_school):
    """1. ë‹¤ë³€ëŸ‰ ì°¨ì› ì¶•ì†Œ ë° ì ì¬ ìš”ì¸ ë¶„ì„ (PCA/Factor Analysis)"""
    print("ğŸ”¬ [1/5] ë‹¤ë³€ëŸ‰ ì ì¬ ìš”ì¸ ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
    
    # ë¶„ì„ìš© ì§€í‘œ ì„ íƒ
    features = ['ì‹¤ì œë°°ì •ì¸ì›', 'ì¼ì§€ë§_ë°°ì •ëœ_ì‚¬ëŒ', 'ì´_1ì§€ë§_ì§€ì›ììˆ˜', 'ì‹¤ì§ˆê²½ìŸë¥ ', 'ë°°ì •ë§Œì¡±ë„(%)']
    x = df_school[features].fillna(0)
    
    # ë°ì´í„° í‘œì¤€í™”
    scaler = StandardScaler()
    x_scaled = scaler.fit_transform(x)
    
    # PCA ìˆ˜í–‰ (2ê°œ ì£¼ì„±ë¶„)
    pca = PCA(n_components=2)
    pca_result = pca.fit_transform(x_scaled)
    
    df_school['PCA_1'] = pca_result[:, 0]
    df_school['PCA_2'] = pca_result[:, 1]
    
    # ìš”ì¸ ë¶€í•˜ëŸ‰(Factor Loadings) í™•ì¸
    loadings = pd.DataFrame(pca.components_.T, columns=['PC1', 'PC2'], index=features)
    
    return df_school, loadings

def analysis_gmm_clustering(df_school):
    """2. í™•ë¥ ì  ëª¨ë¸ ê¸°ë°˜ êµ°ì§‘í™” (Gaussian Mixture Model)"""
    print("ğŸ”¬ [2/5] GMM ê¸°ë°˜ í™•ë¥ ì  í•™êµ ìœ í˜•í™” ìˆ˜í–‰ ì¤‘...")
    
    features = ['PCA_1', 'PCA_2']
    x = df_school[features]
    
    # GMM ìˆ˜í–‰ (ìµœì  êµ°ì§‘ ìˆ˜ëŠ” BICë¡œ ê²°ì •í•  ìˆ˜ ìˆìœ¼ë‚˜ ì—¬ê¸°ì„  4ê°œë¡œ ê°€ì •)
    gmm = GaussianMixture(n_components=4, random_state=42)
    df_school['GMM_Cluster'] = gmm.fit_predict(x)
    df_school['GMM_Probability'] = gmm.predict_proba(x).max(axis=1) # ì†Œì† í™•ë¥ 
    
    return df_school

def analysis_entropy_diversity(df_matrix):
    """3. ì •ë³´ ì—”íŠ¸ë¡œí”¼ë¥¼ ì´ìš©í•œ ì§€ë§/ë°°ì • ë‹¤ì–‘ì„± ë¶„ì„"""
    print("ğŸ”¬ [3/5] ì •ë³´ ì—”íŠ¸ë¡œí”¼(Shannon Entropy) ë‹¤ì–‘ì„± ì§€ìˆ˜ ì‚°ì¶œ ì¤‘...")
    
    # í–‰ë³„(ë™ë„¤ë³„) ì—”íŠ¸ë¡œí”¼ ê³„ì‚°: íŠ¹ì • ë™ë„¤ í•™ìƒë“¤ì´ ì–¼ë§ˆë‚˜ ë‹¤ì–‘í•œ í•™êµë¡œ í©ì–´ì§€ëŠ”ê°€?
    # ê°’ì´ ë‚®ì„ìˆ˜ë¡ íŠ¹ì • í•™êµë¡œì˜ ë°°ì • ì ë¦¼(Segregation)ì´ ê°•í•¨
    dong_entropy = df_matrix.apply(lambda x: entropy(x + 1e-9), axis=1) # 0 ë°©ì§€
    
    # ì—´ë³„(í•™êµë³„) ì—”íŠ¸ë¡œí”¼ ê³„ì‚°: íŠ¹ì • í•™êµê°€ ì–¼ë§ˆë‚˜ ë‹¤ì–‘í•œ ë™ë„¤ì—ì„œ í•™ìƒì„ ë°›ì•„ë“¤ì´ëŠ”ê°€?
    school_entropy = df_matrix.apply(lambda x: entropy(x + 1e-9), axis=0)
    
    df_dong_entropy = pd.DataFrame({'í–‰ì •ë™': dong_entropy.index, 'ì—”íŠ¸ë¡œí”¼_ì§€ìˆ˜': dong_entropy.values})
    df_school_entropy = pd.DataFrame({'ë°°ì •ê³ ë“±í•™êµ': school_entropy.index, 'í¬ìš©ì„±_ì§€ìˆ˜': school_entropy.values})
    
    return df_dong_entropy, df_school_entropy

def analysis_network_centrality(df_matrix):
    """4. ë„¤íŠ¸ì›Œí¬ ë¶„ì„ (Centrality Analysis)"""
    print("ğŸ”¬ [4/5] ì§€ì—­-í•™êµ ë„¤íŠ¸ì›Œí¬ ì¤‘ì‹¬ì„± ë¶„ì„ ì¤‘...")
    
    # ì´ë¶„ ê·¸ë˜í”„(Bipartite Graph) ë˜ëŠ” ê°€ì¤‘ì¹˜ ë°©í–¥ ê·¸ë˜í”„ ìƒì„±
    G = nx.Graph()
    
    for dong in df_matrix.index:
        for school in df_matrix.columns:
            weight = df_matrix.loc[dong, school]
            if weight > 0:
                G.add_edge(dong, school, weight=weight)
    
    # ê³ ìœ ë²¡í„° ì¤‘ì‹¬ì„±(Eigenvector Centrality): ì¤‘ìš”ë„ ì „ì´ ëª¨ë¸
    try:
        centrality = nx.eigenvector_centrality(G, weight='weight', max_iter=1000)
    except:
        centrality = nx.degree_centrality(G) # ìˆ˜ë ´ ì‹¤íŒ¨ ì‹œ ì°¨ìˆ˜ ì¤‘ì‹¬ì„±
        
    df_centrality = pd.DataFrame(list(centrality.items()), columns=['ID', 'ì¤‘ì‹¬ì„±_ì§€ìˆ˜'])
    
    return df_centrality

def analysis_gravity_proxy(df_matrix):
    """5. ê³µê°„ ìƒí˜¸ì‘ìš© í”„ë¡ì‹œ ë¶„ì„ (Interaction Intensity)"""
    print("ğŸ”¬ [5/5] ê³µê°„ ìƒí˜¸ì‘ìš© ê°•ë„ ëª¨ë¸ë§ ì¤‘...")
    
    # ì‹¤ì œ ê±°ë¦¬ëŠ” ë°ì´í„°ì— ì—†ìœ¼ë¯€ë¡œ, ë°°ì • ì¸ì›ì˜ ì§‘ì¤‘ë„ë¥¼ í†µí•´ 'ê³µê°„ì  ë°°íƒ€ì„±'ì„ ì¶”ì •
    # íŠ¹ì • ë™ë„¤ iì™€ í•™êµ j ì‚¬ì´ì˜ ìƒí˜¸ì‘ìš© ê°•ë„ I_ij = T_ij / (sum_i * sum_j)
    total_students = df_matrix.values.sum()
    row_sums = df_matrix.sum(axis=1)
    col_sums = df_matrix.sum(axis=0)
    
    expected = np.outer(row_sums, col_sums) / total_students
    interaction_ratio = df_matrix.values / (expected + 1e-9)
    
    df_interaction = pd.DataFrame(interaction_ratio, index=df_matrix.index, columns=df_matrix.columns)
    
    return df_interaction

def main():
    print("ğŸš€ [Advanced Analytics Engine] ëŒ€í•™ì› ìˆ˜ì¤€ ì‹¬ì¸µ ë¶„ì„ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    try:
        df_school, df_matrix = load_data()
        
        # 1 & 2. PCA + GMM
        df_school, loadings = analysis_pca_factor(df_school)
        df_school = analysis_gmm_clustering(df_school)
        
        # 3. Entropy
        df_dong_entropy, df_school_entropy = analysis_entropy_diversity(df_matrix)
        
        # 4. Network
        df_centrality = analysis_network_centrality(df_matrix)
        
        # 5. Gravity/Interaction
        df_interaction = analysis_gravity_proxy(df_matrix)
        
        # ê²°ê³¼ ì €ì¥
        print(f"ğŸ’¾ ê²°ê³¼ë¥¼ ì €ì¥ ì¤‘ì…ë‹ˆë‹¤: {OUTPUT_EXCEL}")
        with pd.ExcelWriter(OUTPUT_EXCEL, engine='openpyxl') as writer:
            df_school.to_excel(writer, sheet_name='1_í•™êµ_ê³ ê¸‰ìœ í˜•í™”', index=False)
            loadings.to_excel(writer, sheet_name='1_PCA_ë¶€í•˜ëŸ‰')
            df_dong_entropy.to_excel(writer, sheet_name='2_ì§€ì—­_ë°°ì •ë‹¤ì–‘ì„±', index=False)
            df_school_entropy.to_excel(writer, sheet_name='2_í•™êµ_ìˆ˜ìš©ë‹¤ì–‘ì„±', index=False)
            df_centrality.to_excel(writer, sheet_name='3_ë„¤íŠ¸ì›Œí¬_ì¤‘ì‹¬ì„±', index=False)
            df_interaction.to_excel(writer, sheet_name='4_ê³µê°„ìƒí˜¸ì‘ìš©_ê°•ë„')
            
        print("\nâœ¨ ëª¨ë“  ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ê³ ì°¨ì› í†µê³„ ì§€í‘œê°€ Step 4 íŒŒì¼ì— ë°˜ì˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ë„ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
