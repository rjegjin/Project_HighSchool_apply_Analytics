import pandas as pd
import numpy as np
from scipy.stats import chi2_contingency, pearsonr
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import os

# ==========================================
# [ì„¤ì •] ì…ë ¥ ì–‘ì‹ ìˆ˜ì • (ë‹¨ì¼ ì—‘ì…€ íŒŒì¼ ë¡œë“œ)
# ==========================================
# ì—¬ëŸ¬ ê°œì˜ csv íŒŒì¼ ëŒ€ì‹ , ì• ë‹¨ê³„ì—ì„œ ë§Œë“  ì—‘ì…€ íŒŒì¼ í•˜ë‚˜ë§Œ ìˆìœ¼ë©´ ë©ë‹ˆë‹¤.
INPUT_EXCEL = "2025_í›„ê¸°ê³ _ì‹¬ì¸µì—°êµ¬ë³´ê³ ì„œ.xlsx"
OUTPUT_FILE = "2025_í›„ê¸°ê³ _í†µê³„ì _ì‹¬ì¸µë¶„ì„_ì‹ ë¢°ë„ê²€ì¦.xlsx"

# [ì¤‘ìš”] ìµœì†Œ í‘œë³¸ ê¸°ì¤€ (ì´ ìˆ«ìë³´ë‹¤ ì ìœ¼ë©´ í†µê³„ ë¶„ì„ì—ì„œ ì œì™¸)
MIN_SAMPLE_SCHOOL = 10  # í•™êµë³„ ìµœì†Œ ë°°ì • ì¸ì›
MIN_SAMPLE_DONG = 10    # ë™ë„¤ë³„ ìµœì†Œ ê±°ì£¼ í•™ìƒ ìˆ˜

def run_advanced_stats_v2():
    print("ğŸ”¬ ì‹ ë¢°ë„ ê²€ì¦ì´ í¬í•¨ëœ ì‹¬ì¸µ í†µê³„ ì—°êµ¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

    # 1. ë°ì´í„° ë¡œë“œ (ìˆ˜ì •ëœ ë¶€ë¶„: read_csv -> read_excel)
    if not os.path.exists(INPUT_EXCEL):
        print(f"âŒ ì˜¤ë¥˜: '{INPUT_EXCEL}' íŒŒì¼ì´ í´ë”ì— ì—†ìŠµë‹ˆë‹¤.")
        return

    try:
        # ì—‘ì…€ íŒŒì¼ í•˜ë‚˜ì—ì„œ í•„ìš”í•œ 'ì‹œíŠ¸(Sheet)'ë¥¼ ì™ì™ ë½‘ì•„ì˜µë‹ˆë‹¤.
        df_school = pd.read_excel(INPUT_EXCEL, sheet_name='ì—°êµ¬1_í•™êµë³„_ì¸ê¸°ë„')
        df_matrix = pd.read_excel(INPUT_EXCEL, sheet_name='ë¶€ë¡_ë™ë„¤_í•™êµ_ì „ì²´ë§¤íŠ¸ë¦­ìŠ¤', index_col=0)
        print("âœ” ì—‘ì…€ íŒŒì¼ ë°ì´í„° ë¡œë“œ ì„±ê³µ!")
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("   -> ì—‘ì…€ íŒŒì¼ì´ ì—´ë ¤ìˆë‹¤ë©´ ë‹«ê³  ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return

    # ---------------------------------------------------------
    # [Pre-Step] ë°ì´í„° ì‹ ë¢°ë„ í•„í„°ë§
    # ---------------------------------------------------------
    print(f"\nğŸ” ë°ì´í„° ì¶©ë¶„ì„± ê²€ì‚¬ (ê¸°ì¤€: í•™êµ {MIN_SAMPLE_SCHOOL}ëª…, ë™ë„¤ {MIN_SAMPLE_DONG}ëª… ì´ìƒ)")
    
    # í•™êµ í•„í„°ë§
    valid_schools = df_school[df_school['ì‹¤ì œë°°ì •ì¸ì›'] >= MIN_SAMPLE_SCHOOL].copy()
    dropped_schools = len(df_school) - len(valid_schools)
    print(f"   - í•™êµ: ì „ì²´ {len(df_school)}ê°œ ì¤‘ {len(valid_schools)}ê°œ ë¶„ì„ í¬í•¨ ({dropped_schools}ê°œ ì œì™¸ë¨)")

    # í–‰ì •ë™ í•„í„°ë§ (ë§¤íŠ¸ë¦­ìŠ¤ì—ì„œ í–‰ í•©ê³„ ê³„ì‚°)
    dong_counts = df_matrix.sum(axis=1)
    valid_dongs_idx = dong_counts[dong_counts >= MIN_SAMPLE_DONG].index
    
    # ë§¤íŠ¸ë¦­ìŠ¤ ì¬êµ¬ì„± (ìœ íš¨í•œ ë™ë„¤ x ìœ íš¨í•œ í•™êµ)
    # í•™êµ ì»¬ëŸ¼ëª…ì´ ë§¤íŠ¸ë¦­ìŠ¤ ì»¬ëŸ¼ê³¼ ì¼ì¹˜í•œë‹¤ê³  ê°€ì •
    valid_school_names = valid_schools['ë°°ì •ê³ ë“±í•™êµ'].unique()
    # ë§¤íŠ¸ë¦­ìŠ¤ì— ìˆëŠ” í•™êµë§Œ êµì§‘í•©ìœ¼ë¡œ ì„ íƒ
    common_schools = [s for s in valid_school_names if s in df_matrix.columns]
    
    filtered_matrix = df_matrix.loc[valid_dongs_idx, common_schools]
    print(f"   - í–‰ì •ë™: ì „ì²´ {len(df_matrix)}ê°œ ì¤‘ {len(valid_dongs_idx)}ê°œ ë¶„ì„ í¬í•¨")

    if len(valid_schools) < 3 or filtered_matrix.empty:
        print("âš  ê²½ê³ : ë¶„ì„í•  ìˆ˜ ìˆëŠ” ìœ íš¨ ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤. ê¸°ì¤€(MIN_SAMPLE)ì„ ë‚®ì¶°ë³´ì„¸ìš”.")
        return

    # ---------------------------------------------------------
    # [ì—°êµ¬ 1] K-Means êµ°ì§‘ ë¶„ì„ (ìœ íš¨ í•™êµ ëŒ€ìƒ)
    # ---------------------------------------------------------
    print("\nğŸ“Š 1. í•™êµ ìœ í˜•í™” (Clustering) - ìœ íš¨ ë°ì´í„°ë§Œ")
    
    features = valid_schools[['ì‹¤ì§ˆê²½ìŸë¥ ', 'ë°°ì •ë§Œì¡±ë„(%)']].fillna(0)
    scaler = StandardScaler()
    scaled_features = scaler.fit_transform(features)
    
    # ë°ì´í„°ê°€ ì ìœ¼ë©´ í´ëŸ¬ìŠ¤í„° ìˆ˜ë„ ì¤„ì„
    n_clusters = 3 if len(valid_schools) > 10 else 2
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    valid_schools['êµ°ì§‘_Label'] = kmeans.fit_predict(scaled_features)
    
    cluster_summary = valid_schools.groupby('êµ°ì§‘_Label')[['ì‹¤ì§ˆê²½ìŸë¥ ', 'ë°°ì •ë§Œì¡±ë„(%)']].mean().reset_index()
    
    # êµ°ì§‘ ì´ë¦„ ë¶€ì—¬
    def name_cluster(row):
        comp = row['ì‹¤ì§ˆê²½ìŸë¥ ']
        sat = row['ë°°ì •ë§Œì¡±ë„(%)']
        mean_comp = cluster_summary['ì‹¤ì§ˆê²½ìŸë¥ '].mean()
        mean_sat = cluster_summary['ë°°ì •ë§Œì¡±ë„(%)'].mean()
        
        if comp > mean_comp and sat < mean_sat: return "ìœ í˜•A: ê³ ê²½ìŸ_ì•„ì‰¬ì›€"
        elif comp < mean_comp and sat > mean_sat: return "ìœ í˜•B: ì•ˆì •_ë§Œì¡±í˜•"
        elif sat > 90: return "ìœ í˜•C: ê³ ë§Œì¡±_ì ì •í˜•"
        else: return "ìœ í˜•D: ë³µí•©í˜•"

    cluster_summary['ìœ í˜•_íŠ¹ì„±'] = cluster_summary.apply(name_cluster, axis=1)
    label_map = dict(zip(cluster_summary['êµ°ì§‘_Label'], cluster_summary['ìœ í˜•_íŠ¹ì„±']))
    valid_schools['ë¶„ì„_í•™êµìœ í˜•'] = valid_schools['êµ°ì§‘_Label'].map(label_map)

    # ---------------------------------------------------------
    # [ì—°êµ¬ 2] ì¹´ì´ì œê³± ê²€ì • (ìœ íš¨ ë™ë„¤ x ìœ íš¨ í•™êµ)
    # ---------------------------------------------------------
    print("ğŸ“Š 2. ê±°ì£¼ì§€-ë°°ì •í•™êµ ì¢…ì†ì„± ê²€ì • (Filtered)")
    
    # ë¹ˆë„ê°€ 0ì¸ ì»¬ëŸ¼/í–‰ ì œê±° (ì˜¤ë¥˜ ë°©ì§€)
    filtered_matrix = filtered_matrix.loc[:, (filtered_matrix != 0).any(axis=0)]
    
    if filtered_matrix.size > 0:
        chi2, p, dof, expected = chi2_contingency(filtered_matrix)
        n = filtered_matrix.sum().sum()
        min_dim = min(filtered_matrix.shape) - 1
        cramer_v = np.sqrt(chi2 / (n * min_dim)) if min_dim > 0 else 0
        
        chi_msg = "í†µê³„ì  ìœ ì˜í•¨ (ë¯¿ì„ë§Œí•¨)" if p < 0.05 else "ìš°ì—°ì¼ ê°€ëŠ¥ì„± ë†’ìŒ"
    else:
        chi2, p, cramer_v, chi_msg = 0, 1, 0, "ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ë¶„ì„ ë¶ˆê°€"

    chi_result = pd.DataFrame({
        "ë¶„ì„ ëŒ€ìƒ": [f"ë™ë„¤ {len(valid_dongs_idx)}ê°œ x í•™êµ {len(common_schools)}ê°œ"],
        "P-value": [p],
        "ê²°ë¡ ": [chi_msg],
        "ì—°ê´€ì„± ê°•ë„(V)": [cramer_v]
    })

    # ---------------------------------------------------------
    # [ì—°êµ¬ 3] ìƒê´€ê´€ê³„ ë¶„ì„ (ê°€ì¤‘ì¹˜ ê³ ë ¤ ì—†ì´ ìœ íš¨ ë°ì´í„°ë§Œ)
    # ---------------------------------------------------------
    print("ğŸ“Š 3. ê²½ìŸë¥ -ë§Œì¡±ë„ ìƒê´€ê´€ê³„ (Filtered)")
    
    if len(valid_schools) > 2:
        corr, p_val = pearsonr(valid_schools['ì‹¤ì§ˆê²½ìŸë¥ '], valid_schools['ë°°ì •ë§Œì¡±ë„(%)'])
        corr_msg = "ê°•í•œ ìŒì˜ ìƒê´€ê´€ê³„" if corr < -0.5 else "ì•½í•œ ìƒê´€ê´€ê³„"
    else:
        corr, p_val, corr_msg = 0, 1, "ë°ì´í„° ë¶€ì¡±"

    corr_result = pd.DataFrame({
        "ë¶„ì„ í•™êµ ìˆ˜": [len(valid_schools)],
        "ìƒê´€ê³„ìˆ˜(r)": [corr],
        "P-value": [p_val],
        "í•´ì„": [corr_msg]
    })

    # ---------------------------------------------------------
    # ê²°ê³¼ ì €ì¥
    # ---------------------------------------------------------
    with pd.ExcelWriter(OUTPUT_FILE, engine='openpyxl') as writer:
        valid_schools.sort_values('êµ°ì§‘_Label').to_excel(writer, sheet_name='1_ìœ í˜•í™”(ì‹ ë¢°ë°ì´í„°)', index=False)
        cluster_summary.to_excel(writer, sheet_name='1_êµ°ì§‘ìš”ì•½', index=False)
        chi_result.to_excel(writer, sheet_name='2_ì¢…ì†ì„±ê²€ì •_ê²°ê³¼')
        corr_result.to_excel(writer, sheet_name='3_ìƒê´€ê´€ê³„_ê²°ê³¼')
        
        # ì œì™¸ëœ í•™êµ ëª©ë¡ë„ ë³„ë„ ì €ì¥ (ì°¸ê³ ìš©)
        excluded = df_school[~df_school.index.isin(valid_schools.index)]
        if not excluded.empty:
            excluded.to_excel(writer, sheet_name='ë¶€ë¡_ì œì™¸ëœ_ì†Œìˆ˜ë°ì´í„°', index=False)

    print(f"\nâœ… ì‹ ë¢°ë„ ê²€ì¦ ì™„ë£Œ! íŒŒì¼ ìƒì„±ë¨: {OUTPUT_FILE}")
    print(f"   -> ë¶„ì„ì— ì‚¬ìš©ëœ í•™êµ ìˆ˜: {len(valid_schools)} (ì œì™¸ {dropped_schools})")

if __name__ == "__main__":
    run_advanced_stats_v2()